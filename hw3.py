from __future__ import print_function
"""HW3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17PDfkD0UupkkWm7O0M4HnHBt9PqjjcHa

Update lib
"""

#!pip install statsmodels -U

"""HW3 Data Loader"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import random
from statsmodels.tsa.arima_process import arma_generate_sample
import matplotlib.pyplot as plt

from torch.autograd import Variable
from sklearn.preprocessing import MinMaxScaler


#Hw3 Data Loader Code
PLOT = False 
LENGTH = 1000
trainLength = 100
testLength = 10
seq_length = 5

def print_shape(t, title=None):
    if (title is not None):
        print("{}:\t".format(title), end="")
    if (isinstance(t, np.ndarray)):
        print(t.shape)
        return

    if (isinstance(t, list)):
        t = np.asarray(t)
        print(t.shape)
        return
    print(t.detach().numpy().shape)

arparams = np.array([0.6,-0.5,-0.2])
maparams = np.array([])
noise = 0.1

# Generate train and test sequences
ar = np.r_[1, -arparams]  # add zero-lag and negate
ma = np.r_[1, maparams]  # add zero-lag

np.random.seed(1000)
randomGenerator = np.random.rand # ~Uniform[0,1]

trainData = arma_generate_sample(ar=ar, ma=ma, nsample=trainLength, scale=noise, distrvs=np.random.uniform)
testData = arma_generate_sample(ar=ar, ma=ma, nsample=testLength, scale=noise, distrvs=np.random.uniform)

print_shape(trainData, title="Train Data:")
print_shape(testData, title="Test Data:")

if (PLOT):
    fig, axs = plt.subplots(2, 1, figsize=(18, 6))
    axs[0].plot(trainData[1:200])
    axs[0].grid(True)
    axs[1].plot(testData[1:200])
    axs[1].grid(True)
    plt.show()

"""LSTM Model"""

class LSTM(nn.Module):

    def __init__(self, num_classes, input_size, hidden_size, num_layers):
        super(LSTM, self).__init__()
        
        self.num_classes = num_classes
        self.num_layers = num_layers
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.seq_length = seq_length
        
        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,
                            num_layers=num_layers, batch_first=True)
        
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        h_0 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_size))
        
        c_0 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_size))
        
        # Propagate input through LSTM
        ula, (h_out, _) = self.lstm(x, (h_0, c_0))
        
        h_out = h_out.view(-1, self.hidden_size)
        
        out = self.fc(h_out)
        
        return out

"""Data Loading"""

def sliding_windows(data, seq_length):
    x = []
    y = []

    for i in range(len(data)-seq_length-1):
        _x = data[i:(i+seq_length)]
        _y = data[i+seq_length]
        x.append(_x)
        y.append(_y)

    return np.array(x),np.array(y)

sc = MinMaxScaler()
training_data_norm = sc.fit_transform(np.reshape(trainData,(100,-1)))
test_data_norm = sc.fit_transform(np.reshape(testData,(10,-1)))

#calc the sliding window for the training set => sequence + ground truth output.
x, y = sliding_windows(training_data_norm, seq_length)

dataX = Variable(torch.Tensor(np.array(x)))
dataY = Variable(torch.Tensor(np.array(y)))

trainX = Variable(torch.Tensor(np.array(x[0:trainLength])))
trainY = Variable(torch.Tensor(np.array(y[0:trainLength])))

"""Training Model"""

num_epochs = 2000
learning_rate = 0.01

input_size = 1
hidden_size = 10
num_layers = 1
num_classes = 1

lstm = LSTM(num_classes, input_size, hidden_size, num_layers)

criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)

# Train the model
for epoch in range(num_epochs):
    outputs = lstm(trainX)
    optimizer.zero_grad()
    
    # obtain the loss function
    loss = criterion(outputs, trainY)
    
    loss.backward()
    
    optimizer.step()
    if epoch % 100 == 0:
      print("Epoch: %d, loss: %1.5f" % (epoch, loss.item()))

"""Testing The Model"""

lstm.eval()

x = torch.FloatTensor(testData[-seq_length:])
print(x)
train_predict = lstm(x) #first 5 samples from the test used as input
data_predict = train_predict.data.numpy()
dataY_plot = testData[-seq_length:].data.numpy()

data_predict = sc.inverse_transform(data_predict)
dataY_plot = sc.inverse_transform(dataY_plot)

plt.axvline(x=train_size, c='r', linestyle='--')

plt.plot(dataY_plot)
plt.plot(data_predict)
plt.suptitle('Time-Series Prediction')
plt.show()

'''
lstm.eval()
train_predict = lstm(dataX)

data_predict = train_predict.data.numpy()
dataY_plot = dataY.data.numpy()

data_predict = sc.inverse_transform(data_predict)
dataY_plot = sc.inverse_transform(dataY_plot)

plt.axvline(x=94, c='r', linestyle='--')

plt.plot(dataY_plot)
plt.plot(data_predict)
plt.suptitle('Time-Series Prediction')
plt.show()
'''